{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Synthetic Potential Failures Data Generator\n\n",
        "This notebook generates scalable, configurable synthetic data for `customer_success.app_potential_failures` with the exact output schema requested. It supports:\n\n",
        "- Parameterized generation size (target ~15k rows; adjustable)\n",
        "- KPI code filtering (all or selected); optional non-KPI tasks\n",
        "- Duration profiles (short/medium/long) per KPI with random start/end times\n",
        "- Cross-period boundaries and cross-financial-year spans (per KPI at least one)\n",
        "- Overlapping tasks by station and time window for duplicate-tracking tests\n",
        "- All jobs in COMP status by default; optional status history (WAPPR→APPR→COMP)\n",
        "- Station distribution across all non-null sections from `customer_success.dimStation`\n",
        "- Date enrichment by joining to `core_dimdate` for Period, PeriodWeek, PeriodYear\n",
        "- Start date baseline 25/05/2025 (GTS EL) over a 2-year window\n",
        "- Write-first to lakehouse/local for validation; optional write to SQL Server after validation\n\n",
        "Configuration is controlled via a single config dict below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "import string\n",
        "from datetime import datetime, timedelta\n",
        "from typing import Optional, Sequence, Dict, Any, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import pyodbc  # For SQL Server optional write\n",
        "except Exception:  # keep notebook runnable without driver\n",
        "    pyodbc = None\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration\n",
        "# ---------------------------\n",
        "CONFIG: Dict[str, Any] = {\n",
        "    # Generation controls\n",
        "    \"seed\": 42,\n",
        "    \"target_row_count\": 15000,  # dial up/down\n",
        "    \"start_date\": \"2025-05-25\",  # inclusive\n",
        "    \"months_span\": 24,  # ~2 years window\n",
        "\n",
        "    # KPI controls\n",
        "    \"kpi_include\": \"all\",  # \"all\" or list of codes\n",
        "    \"kpi_exclude\": [],\n",
        "    \"allow_non_kpi\": False,  # nice-to-have optional\n",
        "\n",
        "    # Duration profile per KPI (minutes). Will randomize within buckets.\n",
        "    # You can override by providing a mapping {kpi_code: {\"short\":(min,max),\"medium\":(...),\"long\":(...)}}\n",
        "    \"duration_profiles\": {},\n",
        "    # Probability weights for picking short/medium/long durations\n",
        "    \"duration_weights\": {\"short\": 0.4, \"medium\": 0.4, \"long\": 0.2},\n",
        "\n",
        "    # Overlap control (duplicates testing): number of clusters and tightness (minutes)\n",
        "    \"overlap_clusters_per_kpi\": 3,\n",
        "    \"overlap_window_minutes\": 120,  # events within +/- 2 hours\n",
        "    \"overlap_events_per_cluster\": (2, 6),  # min/max per cluster\n",
        "\n",
        "    # Cross-boundary controls\n",
        "    \"ensure_cross_fin_year_per_kpi\": True,\n",
        "    \"financial_year_end\": \"2026-03-31\",  # UK rail FY can differ; configurable\n",
        "    \"ensure_period_crossing_ratio\": 0.1,  # ~10% of tasks will cross period boundaries\n",
        "\n",
        "    # Station distribution\n",
        "    \"station_distribution_bias\": \"uniform\",  # or 'by_volume' if dimStation has volumes\n",
        "\n",
        "    # Statuses\n",
        "    \"all_comp_status\": True,  # required default\n",
        "    \"include_status_history\": False,  # nice-to-have\n",
        "    \"status_history_steps\": [\"WAPPR\", \"APPR\", \"COMP\"],\n",
        "\n",
        "    # Output controls\n",
        "    \"local_validate_dir\": \"./data/outputs\",\n",
        "    \"write_parquet\": True,\n",
        "    \"write_csv\": False,\n",
        "\n",
        "    # SQL Server write (post-validate)\n",
        "    \"sqlserver_write\": False,\n",
        "    \"sqlserver_dsn\": None,  # e.g., \"Driver={ODBC Driver 17 for SQL Server};Server=...;Database=...;Trusted_Connection=yes;\"\n",
        "    \"sqlserver_table\": \"customer_success.app_potential_failures_synth\",\n",
        "}\n",
        "\n",
        "random.seed(CONFIG[\"seed\"])\n",
        "np.random.seed(CONFIG[\"seed\"])\n",
        "\n",
        "# Make output dir\n",
        "os.makedirs(CONFIG[\"local_validate_dir\"], exist_ok=True)\n",
        "\n",
        "print(\"Config loaded. Seeded RNGs.\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1e622ee8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------\n",
        "# Schema and helpers\n",
        "# ---------------------------\n",
        "SCHEMA_COLUMNS = [\n",
        "    (\"TaskId\", \"string\"),\n",
        "    (\"RecordID\", \"string\"),\n",
        "    (\"Instruction_Code\", \"string\"),\n",
        "    (\"Building\", \"string\"),\n",
        "    (\"BuildingName\", \"string\"),\n",
        "    (\"LocationName\", \"string\"),\n",
        "    (\"ShortDescription\", \"string\"),\n",
        "    (\"LongDescription\", \"string\"),\n",
        "    (\"Reporter\", \"string\"),\n",
        "    (\"ReporterEmail\", \"string\"),\n",
        "    (\"Notes\", \"string\"),\n",
        "    (\"ReportedDate\", \"datetime64[ns]\"),\n",
        "    (\"DueBy\", \"datetime64[ns]\"),\n",
        "    (\"ScheduledFor\", \"datetime64[ns]\"),\n",
        "    (\"Finished\", \"datetime64[ns]\"),\n",
        "    (\"Status\", \"string\"),\n",
        "    (\"LoggedBy\", \"string\"),\n",
        "    (\"LoggedOn\", \"datetime64[ns]\"),\n",
        "    (\"ModifiedOn\", \"datetime64[ns]\"),\n",
        "    (\"SLAStatus\", \"string\"),\n",
        "    (\"CreatedTimestamp\", \"datetime64[ns]\"),\n",
        "    (\"LastUploaded\", \"datetime64[ns]\"),\n",
        "    (\"IsCurrent\", \"bool\"),\n",
        "    (\"Period\", \"string\"),\n",
        "    (\"PeriodWeek\", \"Int64\"),  # pandas nullable int\n",
        "    (\"PeriodYear\", \"Int64\"),\n",
        "    (\"StationSection\", \"string\"),\n",
        "    (\"KPIDescription\", \"string\"),\n",
        "    (\"KPICategory\", \"string\"),\n",
        "]\n",
        "\n",
        "# Some generic helpers\n",
        "\n",
        "def rand_string(prefix: str, n: int = 8) -> str:\n",
        "    return f\"{prefix}-\" + \"\".join(random.choices(string.ascii_uppercase + string.digits, k=n))\n",
        "\n",
        "\n",
        "def choose_weighted(options: Sequence[str], weights: Sequence[float]) -> str:\n",
        "    return random.choices(options, weights=weights, k=1)[0]\n",
        "\n",
        "\n",
        "def to_dt(x: str | datetime) -> datetime:\n",
        "    return x if isinstance(x, datetime) else datetime.fromisoformat(x)\n",
        "\n",
        "\n",
        "START_DATE: datetime = to_dt(CONFIG[\"start_date\"])  # baseline\n",
        "END_DATE: datetime = START_DATE + pd.DateOffset(months=CONFIG[\"months_span\"]).to_pydatetime()  # approx\n",
        "FY_END: datetime = to_dt(CONFIG[\"financial_year_end\"])"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "243fffe5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------\n",
        "# Load dimensions (KPI, Station, Date)\n",
        "# ---------------------------\n",
        "# The notebook supports three strategies:\n",
        "# 1) Load from source database via pyodbc if configured\n",
        "# 2) Load from CSVs placed in ./data/inputs\n",
        "# 3) Fallback synthetic dims with reasonable defaults\n",
        "\n",
        "INPUT_DIR = \"./data/inputs\"\n",
        "os.makedirs(INPUT_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "def load_kpi_dim() -> pd.DataFrame:\n",
        "    # Expectation: 'bronze.fms_dimkpiclassification' has columns including kpi code and description/category\n",
        "    # We'll look for a CSV first, else fabricate a set covering examples like Graffiti, Track Side Cleaning\n",
        "    csv_path = os.path.join(INPUT_DIR, \"fms_dimkpiclassification.csv\")\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        # normalize expected columns\n",
        "        # expected columns: Code, KPIDescription, KPICategory\n",
        "        colmap = {}\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if lc in (\"code\", \"kpicode\", \"instruction_code\"):\n",
        "                colmap[c] = \"Instruction_Code\"\n",
        "            elif lc in (\"kpidescription\", \"description\"):\n",
        "                colmap[c] = \"KPIDescription\"\n",
        "            elif lc in (\"kpicategory\", \"category\"):\n",
        "                colmap[c] = \"KPICategory\"\n",
        "        df = df.rename(columns=colmap)\n",
        "        required = [\"Instruction_Code\", \"KPIDescription\", \"KPICategory\"]\n",
        "        for r in required:\n",
        "            if r not in df.columns:\n",
        "                raise ValueError(f\"KPI dim missing required column: {r}\")\n",
        "        df = df[required].drop_duplicates()\n",
        "        return df\n",
        "\n",
        "    # Fallback: fabricate a representative set of KPI codes\n",
        "    data = [\n",
        "        (\"GRF\", \"Graffiti Removal\", \"Cleaning\"),\n",
        "        (\"TSC\", \"Track Side Cleaning\", \"Cleaning\"),\n",
        "        (\"LIT\", \"Litter Removal\", \"Cleaning\"),\n",
        "        (\"LGT\", \"Lighting Fault\", \"Asset\"),\n",
        "        (\"LFT\", \"Lift Fault\", \"Asset\"),\n",
        "        (\"ESC\", \"Escalator Fault\", \"Asset\"),\n",
        "        (\"WC\", \"Toilet Outage\", \"Facilities\"),\n",
        "        (\"SIG\", \"Signage Damage\", \"Asset\"),\n",
        "        (\"PA\", \"Public Address Fault\", \"Asset\"),\n",
        "        (\"VND\", \"Vending Machine Issue\", \"Retail\"),\n",
        "    ]\n",
        "    return pd.DataFrame(data, columns=[\"Instruction_Code\", \"KPIDescription\", \"KPICategory\"]).drop_duplicates()\n",
        "\n",
        "\n",
        "def load_station_dim() -> pd.DataFrame:\n",
        "    # Expectation: 'customer_success.dimStation' with StationSection non-null to include\n",
        "    csv_path = os.path.join(INPUT_DIR, \"dimStation.csv\")\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        colmap = {}\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if lc in (\"stationsection\", \"section\", \"station_code\", \"station\"):\n",
        "                colmap[c] = \"StationSection\"\n",
        "            elif lc in (\"building\",):\n",
        "                colmap[c] = \"Building\"\n",
        "            elif lc in (\"buildingname\",):\n",
        "                colmap[c] = \"BuildingName\"\n",
        "            elif lc in (\"locationname\", \"location\"):\n",
        "                colmap[c] = \"LocationName\"\n",
        "        df = df.rename(columns=colmap)\n",
        "        if \"StationSection\" not in df.columns:\n",
        "            raise ValueError(\"Station dim missing StationSection\")\n",
        "        df = df[df[\"StationSection\"].notna()].copy()\n",
        "        # keep only needed for generation, allow missing building/name\n",
        "        return df[[c for c in [\"StationSection\", \"Building\", \"BuildingName\", \"LocationName\"] if c in df.columns]].drop_duplicates()\n",
        "\n",
        "    # Fallback synthetic stations\n",
        "    stations = [\n",
        "        (\"STN001\", \"STN001-BLD\", \"Central Station\", \"Platform 1\"),\n",
        "        (\"STN002\", \"STN002-BLD\", \"Riverside\", \"Platform 2\"),\n",
        "        (\"STN003\", \"STN003-BLD\", \"Hilltop\", \"Concourse\"),\n",
        "        (\"STN004\", \"STN004-BLD\", \"Parkway\", \"Entrance A\"),\n",
        "        (\"STN005\", \"STN005-BLD\", \"West End\", \"Platform 4\"),\n",
        "    ]\n",
        "    return pd.DataFrame(stations, columns=[\"StationSection\", \"Building\", \"BuildingName\", \"LocationName\"]).drop_duplicates()\n",
        "\n",
        "\n",
        "def load_date_dim(start: datetime, end: datetime) -> pd.DataFrame:\n",
        "    # Expectation: core_dimdate providing Period, PeriodWeek, PeriodYear per calendar date\n",
        "    csv_path = os.path.join(INPUT_DIR, \"core_dimdate.csv\")\n",
        "    if os.path.exists(csv_path):\n",
        "        df = pd.read_csv(csv_path, parse_dates=[c for c in [\"Date\", \"date\"] if c in pd.read_csv(csv_path, nrows=0).columns])\n",
        "        # normalize\n",
        "        colmap = {}\n",
        "        for c in df.columns:\n",
        "            lc = c.lower()\n",
        "            if lc in (\"date\", \"calendardate\"):\n",
        "                colmap[c] = \"Date\"\n",
        "            elif lc in (\"period\", \"railperiod\"):\n",
        "                colmap[c] = \"Period\"\n",
        "            elif lc in (\"periodweek\", \"railperiodweek\", \"period_week\"):\n",
        "                colmap[c] = \"PeriodWeek\"\n",
        "            elif lc in (\"periodyear\", \"railperiodyear\", \"period_year\"):\n",
        "                colmap[c] = \"PeriodYear\"\n",
        "        df = df.rename(columns=colmap)\n",
        "        required = [\"Date\", \"Period\", \"PeriodWeek\", \"PeriodYear\"]\n",
        "        for r in required:\n",
        "            if r not in df.columns:\n",
        "                raise ValueError(f\"Date dim missing required column: {r}\")\n",
        "        # filter to range\n",
        "        mask = (df[\"Date\"] >= start.date()) & (df[\"Date\"] <= end.date())\n",
        "        return df.loc[mask, required].drop_duplicates().sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    # Fallback synthetic: build daily rows with simple 4-week periods and 13 periods per FY\n",
        "    dates = pd.date_range(start=start.date(), end=end.date(), freq=\"D\")\n",
        "\n",
        "    def rail_period(d: pd.Timestamp) -> Tuple[str, int, int]:\n",
        "        # Simplified: FY starts April 1, 13 periods of 28 days each\n",
        "        fy_year = d.year if d >= pd.Timestamp(year=d.year, month=4, day=1) else d.year - 1\n",
        "        epoch = pd.Timestamp(year=fy_year, month=4, day=1)\n",
        "        delta_days = (d - epoch).days\n",
        "        period_index = (delta_days // 28) % 13 + 1\n",
        "        week_in_period = (delta_days % 28) // 7 + 1\n",
        "        period_label = f\"P{period_index:02d}/{fy_year}\"\n",
        "        return period_label, week_in_period, fy_year\n",
        "\n",
        "    rows = []\n",
        "    for d in dates:\n",
        "        p, w, y = rail_period(d)\n",
        "        rows.append({\"Date\": d.normalize(), \"Period\": p, \"PeriodWeek\": w, \"PeriodYear\": y})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "kpi_dim = load_kpi_dim()\n",
        "station_dim = load_station_dim()\n",
        "date_dim = load_date_dim(START_DATE, END_DATE)\n",
        "\n",
        "print(f\"Loaded KPI: {len(kpi_dim)} codes; Stations: {len(station_dim)}; Dates: {len(date_dim)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6b694382"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------\n",
        "# KPI selection and duration profiles\n",
        "# ---------------------------\n",
        "# Filter to KPI codes only, as requested (exclude non-KPI)\n",
        "\n",
        "if CONFIG[\"kpi_include\"] == \"all\":\n",
        "    selected_kpis = kpi_dim[\"Instruction_Code\"].tolist()\n",
        "else:\n",
        "    selected_kpis = [k for k in kpi_dim[\"Instruction_Code\"].tolist() if k in set(CONFIG[\"kpi_include\"])]\n",
        "\n",
        "selected_kpis = [k for k in selected_kpis if k not in set(CONFIG[\"kpi_exclude\"])]\n",
        "\n",
        "# Build duration profiles for each KPI code\n",
        "DEFAULT_DURATION_PROFILES = {\n",
        "    # minutes: ranges are inclusive of variability\n",
        "    \"short\": (15, 120),\n",
        "    \"medium\": (121, 480),\n",
        "    \"long\": (481, 2880),  # up to 2 days for a single task\n",
        "}\n",
        "\n",
        "kpi_to_profiles: Dict[str, Dict[str, Tuple[int, int]]] = {}\n",
        "for code in selected_kpis:\n",
        "    override = CONFIG[\"duration_profiles\"].get(code) if isinstance(CONFIG.get(\"duration_profiles\"), dict) else None\n",
        "    if override:\n",
        "        prof = {k: tuple(v) for k, v in override.items()}\n",
        "    else:\n",
        "        prof = DEFAULT_DURATION_PROFILES.copy()\n",
        "    kpi_to_profiles[code] = prof\n",
        "\n",
        "print(f\"Selected KPI codes: {len(selected_kpis)}\")"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "f3bc53f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------\n",
        "# Core generators\n",
        "# ---------------------------\n",
        "\n",
        "def random_duration_minutes(code: str) -> int:\n",
        "    buckets = list(CONFIG[\"duration_weights\"].keys())\n",
        "    weights = list(CONFIG[\"duration_weights\"].values())\n",
        "    bucket = choose_weighted(buckets, weights)\n",
        "    low, high = kpi_to_profiles[code][bucket]\n",
        "    return random.randint(low, high)\n",
        "\n",
        "\n",
        "def random_datetime_within_window(start: datetime, end: datetime) -> datetime:\n",
        "    total_seconds = int((end - start).total_seconds())\n",
        "    if total_seconds <= 0:\n",
        "        return start\n",
        "    offset = random.randint(0, total_seconds)\n",
        "    return start + timedelta(seconds=offset)\n",
        "\n",
        "\n",
        "def generate_task_row(code: str, station: pd.Series, date_dim: pd.DataFrame) -> Dict[str, Any]:\n",
        "    # base times\n",
        "    reported = random_datetime_within_window(START_DATE, END_DATE)\n",
        "    duration_min = random_duration_minutes(code)\n",
        "    start_time = reported + timedelta(minutes=random.randint(0, 240))  # start after reported\n",
        "    finish_time = start_time + timedelta(minutes=duration_min)\n",
        "    due_by = reported + timedelta(hours=random.randint(6, 72))\n",
        "    scheduled_for = reported + timedelta(hours=random.randint(1, 48))\n",
        "\n",
        "    # join to date dim for Period fields using date component of LoggedOn or ReportedDate\n",
        "    logged_on_date = pd.Timestamp(reported.date())\n",
        "    dd = date_dim.loc[date_dim[\"Date\"] == logged_on_date]\n",
        "    if dd.empty:\n",
        "        dd = date_dim.iloc[[random.randrange(0, len(date_dim))]]\n",
        "\n",
        "    period = dd.iloc[0][\"Period\"]\n",
        "    period_week = int(dd.iloc[0][\"PeriodWeek\"]) if not pd.isna(dd.iloc[0][\"PeriodWeek\"]) else None\n",
        "    period_year = int(dd.iloc[0][\"PeriodYear\"]) if not pd.isna(dd.iloc[0][\"PeriodYear\"]) else None\n",
        "\n",
        "    # descriptions\n",
        "    kpi_row = kpi_dim.loc[kpi_dim[\"Instruction_Code\"] == code].iloc[0]\n",
        "    short_desc = f\"{kpi_row['KPIDescription']} at {station['StationSection']}\"\n",
        "    long_desc = f\"{short_desc}. Duration approx {duration_min} minutes.\"\n",
        "\n",
        "    status = \"COMP\" if CONFIG[\"all_comp_status\"] else choose_weighted([\"WAPPR\", \"APPR\", \"COMP\"], [0.05, 0.15, 0.8])\n",
        "\n",
        "    row = {\n",
        "        \"TaskId\": rand_string(\"TASK\"),\n",
        "        \"RecordID\": rand_string(\"REC\"),\n",
        "        \"Instruction_Code\": code,\n",
        "        \"Building\": station.get(\"Building\", None),\n",
        "        \"BuildingName\": station.get(\"BuildingName\", None),\n",
        "        \"LocationName\": station.get(\"LocationName\", None),\n",
        "        \"ShortDescription\": short_desc,\n",
        "        \"LongDescription\": long_desc,\n",
        "        \"Reporter\": random.choice([\"John Smith\", \"Jane Doe\", \"Ops Bot\", \"Station Manager\", \"Anonymous\"]),\n",
        "        \"ReporterEmail\": random.choice([\"john@example.com\", \"jane@example.com\", \"ops@example.com\", None]),\n",
        "        \"Notes\": random.choice([None, \"Urgent\", \"Follow-up required\", \"Photographic evidence attached\"]),\n",
        "        \"ReportedDate\": reported,\n",
        "        \"DueBy\": due_by,\n",
        "        \"ScheduledFor\": scheduled_for,\n",
        "        \"Finished\": finish_time,\n",
        "        \"Status\": status,\n",
        "        \"LoggedBy\": random.choice([\"OpsConsole\", \"MobileApp\", \"WebPortal\"]),\n",
        "        \"LoggedOn\": reported,\n",
        "        \"ModifiedOn\": finish_time,\n",
        "        \"SLAStatus\": random.choice([\"MET\", \"BREACH\", \"AT_RISK\"]),\n",
        "        \"CreatedTimestamp\": reported,\n",
        "        \"LastUploaded\": finish_time,\n",
        "        \"IsCurrent\": True,\n",
        "        \"Period\": period,\n",
        "        \"PeriodWeek\": period_week,\n",
        "        \"PeriodYear\": period_year,\n",
        "        \"StationSection\": station[\"StationSection\"],\n",
        "        \"KPIDescription\": kpi_row[\"KPIDescription\"],\n",
        "        \"KPICategory\": kpi_row[\"KPICategory\"],\n",
        "    }\n",
        "    return row\n",
        "\n",
        "\n",
        "def force_cross_fin_year_case(code: str, station: pd.Series) -> Dict[str, Any]:\n",
        "    # Start before FY end and finish after FY end\n",
        "    start_before = FY_END - timedelta(days=random.randint(1, 14))\n",
        "    # ensure spanning many hours/days\n",
        "    finish_after = FY_END + timedelta(days=random.randint(1, 30), hours=random.randint(0, 23))\n",
        "\n",
        "    # Use reported/logged around start_before to keep consistency\n",
        "    reported = start_before - timedelta(hours=random.randint(0, 48))\n",
        "\n",
        "    # Join Period by reported date\n",
        "    dd = date_dim.loc[date_dim[\"Date\"] == pd.Timestamp(reported.date())]\n",
        "    if dd.empty:\n",
        "        dd = date_dim.iloc[[random.randrange(0, len(date_dim))]]\n",
        "\n",
        "    kpi_row = kpi_dim.loc[kpi_dim[\"Instruction_Code\"] == code].iloc[0]\n",
        "\n",
        "    return {\n",
        "        \"TaskId\": rand_string(\"TASK\"),\n",
        "        \"RecordID\": rand_string(\"REC\"),\n",
        "        \"Instruction_Code\": code,\n",
        "        \"Building\": station.get(\"Building\", None),\n",
        "        \"BuildingName\": station.get(\"BuildingName\", None),\n",
        "        \"LocationName\": station.get(\"LocationName\", None),\n",
        "        \"ShortDescription\": f\"{kpi_row['KPIDescription']} cross-FY test\",\n",
        "        \"LongDescription\": f\"Spans FY end at {FY_END.date()} for rollover testing.\",\n",
        "        \"Reporter\": \"Station Manager\",\n",
        "        \"ReporterEmail\": \"manager@example.com\",\n",
        "        \"Notes\": \"Cross-year scenario\",\n",
        "        \"ReportedDate\": reported,\n",
        "        \"DueBy\": start_before + timedelta(days=7),\n",
        "        \"ScheduledFor\": start_before,\n",
        "        \"Finished\": finish_after,\n",
        "        \"Status\": \"COMP\",\n",
        "        \"LoggedBy\": \"OpsConsole\",\n",
        "        \"LoggedOn\": reported,\n",
        "        \"ModifiedOn\": finish_after,\n",
        "        \"SLAStatus\": random.choice([\"MET\", \"BREACH\", \"AT_RISK\"]),\n",
        "        \"CreatedTimestamp\": reported,\n",
        "        \"LastUploaded\": finish_after,\n",
        "        \"IsCurrent\": True,\n",
        "        \"Period\": dd.iloc[0][\"Period\"],\n",
        "        \"PeriodWeek\": int(dd.iloc[0][\"PeriodWeek\"]) if not pd.isna(dd.iloc[0][\"PeriodWeek\"]) else None,\n",
        "        \"PeriodYear\": int(dd.iloc[0][\"PeriodYear\"]) if not pd.isna(dd.iloc[0][\"PeriodYear\"]) else None,\n",
        "        \"StationSection\": station[\"StationSection\"],\n",
        "        \"KPIDescription\": kpi_row[\"KPIDescription\"],\n",
        "        \"KPICategory\": kpi_row[\"KPICategory\"],\n",
        "    }\n",
        "\n",
        "\n",
        "def generate_overlap_cluster(code: str, station: pd.Series, center: datetime) -> List[Dict[str, Any]]:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    n = random.randint(*CONFIG[\"overlap_events_per_cluster\"])\n",
        "    for _ in range(n):\n",
        "        # reported around the center time within overlap window\n",
        "        half = CONFIG[\"overlap_window_minutes\"]\n",
        "        reported = center + timedelta(minutes=random.randint(-half, half))\n",
        "        duration_min = random_duration_minutes(code)\n",
        "        start_time = reported + timedelta(minutes=random.randint(0, 90))\n",
        "        finish_time = start_time + timedelta(minutes=duration_min)\n",
        "\n",
        "        dd = date_dim.loc[date_dim[\"Date\"] == pd.Timestamp(reported.date())]\n",
        "        if dd.empty:\n",
        "            dd = date_dim.iloc[[random.randrange(0, len(date_dim))]]\n",
        "        kpi_row = kpi_dim.loc[kpi_dim[\"Instruction_Code\"] == code].iloc[0]\n",
        "\n",
        "        rows.append({\n",
        "            \"TaskId\": rand_string(\"TASK\"),\n",
        "            \"RecordID\": rand_string(\"REC\"),\n",
        "            \"Instruction_Code\": code,\n",
        "            \"Building\": station.get(\"Building\", None),\n",
        "            \"BuildingName\": station.get(\"BuildingName\", None),\n",
        "            \"LocationName\": station.get(\"LocationName\", None),\n",
        "            \"ShortDescription\": f\"{kpi_row['KPIDescription']} possible duplicate\",\n",
        "            \"LongDescription\": \"Intentional temporal overlap for duplicate testing\",\n",
        "            \"Reporter\": random.choice([\"Ops Bot\", \"Station Manager\"]),\n",
        "            \"ReporterEmail\": random.choice([\"ops@example.com\", \"manager@example.com\", None]),\n",
        "            \"Notes\": \"overlap-cluster\",\n",
        "            \"ReportedDate\": reported,\n",
        "            \"DueBy\": reported + timedelta(hours=random.randint(1, 48)),\n",
        "            \"ScheduledFor\": reported + timedelta(hours=random.randint(0, 24)),\n",
        "            \"Finished\": finish_time,\n",
        "            \"Status\": \"COMP\",\n",
        "            \"LoggedBy\": \"MobileApp\",\n",
        "            \"LoggedOn\": reported,\n",
        "            \"ModifiedOn\": finish_time,\n",
        "            \"SLAStatus\": random.choice([\"MET\", \"BREACH\", \"AT_RISK\"]),\n",
        "            \"CreatedTimestamp\": reported,\n",
        "            \"LastUploaded\": finish_time,\n",
        "            \"IsCurrent\": True,\n",
        "            \"Period\": dd.iloc[0][\"Period\"],\n",
        "            \"PeriodWeek\": int(dd.iloc[0][\"PeriodWeek\"]) if not pd.isna(dd.iloc[0][\"PeriodWeek\"]) else None,\n",
        "            \"PeriodYear\": int(dd.iloc[0][\"PeriodYear\"]) if not pd.isna(dd.iloc[0][\"PeriodYear\"]) else None,\n",
        "            \"StationSection\": station[\"StationSection\"],\n",
        "            \"KPIDescription\": kpi_row[\"KPIDescription\"],\n",
        "            \"KPICategory\": kpi_row[\"KPICategory\"],\n",
        "        })\n",
        "    return rows"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "369f9144"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ---------------------------\n",
        "# Main generation routine\n",
        "# ---------------------------\n",
        "\n",
        "def generate_tasks() -> pd.DataFrame:\n",
        "    total = CONFIG[\"target_row_count\"]\n",
        "\n",
        "    # Distribute roughly evenly across KPI codes\n",
        "    per_kpi_base = total // max(1, len(selected_kpis))\n",
        "    surplus = total - per_kpi_base * len(selected_kpis)\n",
        "\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "\n",
        "    # Pre-calc some stations index for speed\n",
        "    stations_idx = station_dim.index.tolist()\n",
        "\n",
        "    # 1) Base distribution\n",
        "    for i, code in enumerate(selected_kpis):\n",
        "        count = per_kpi_base + (1 if i < surplus else 0)\n",
        "        for _ in range(count):\n",
        "            st = station_dim.loc[random.choice(stations_idx)]\n",
        "            rows.append(generate_task_row(code, st, date_dim))\n",
        "\n",
        "    # 2) Ensure cross-financial-year per KPI if requested\n",
        "    if CONFIG[\"ensure_cross_fin_year_per_kpi\"]:\n",
        "        for code in selected_kpis:\n",
        "            st = station_dim.loc[random.choice(stations_idx)]\n",
        "            rows.append(force_cross_fin_year_case(code, st))\n",
        "\n",
        "    # 3) Period crossing ratio: lengthen some tasks to cross period boundaries\n",
        "    if CONFIG[\"ensure_period_crossing_ratio\"] > 0:\n",
        "        n_cross = max(1, int(len(rows) * CONFIG[\"ensure_period_crossing_ratio\"]))\n",
        "        for _ in range(n_cross):\n",
        "            code = random.choice(selected_kpis)\n",
        "            st = station_dim.loc[random.choice(stations_idx)]\n",
        "            # pick a start close to the end of a period date, then long duration\n",
        "            # choose a date from last days of a period\n",
        "            last_days = date_dim.groupby(\"Period\").tail(2)[\"Date\"].tolist()\n",
        "            if last_days:\n",
        "                start_date = random.choice(last_days)\n",
        "            else:\n",
        "                start_date = pd.Timestamp(random_datetime_within_window(START_DATE, END_DATE).date())\n",
        "            reported = datetime.combine(start_date.to_pydatetime().date(), datetime.min.time()) + timedelta(hours=random.randint(0, 20))\n",
        "            # long duration to cross into next period\n",
        "            duration_min = random.randint(24 * 60, 3 * 24 * 60)\n",
        "            start_time = reported + timedelta(minutes=random.randint(0, 180))\n",
        "            finish_time = start_time + timedelta(minutes=duration_min)\n",
        "\n",
        "            dd = date_dim.loc[date_dim[\"Date\"] == start_date]\n",
        "            if dd.empty:\n",
        "                dd = date_dim.iloc[[random.randrange(0, len(date_dim))]]\n",
        "            kpi_row = kpi_dim.loc[kpi_dim[\"Instruction_Code\"] == code].iloc[0]\n",
        "\n",
        "            rows.append({\n",
        "                \"TaskId\": rand_string(\"TASK\"),\n",
        "                \"RecordID\": rand_string(\"REC\"),\n",
        "                \"Instruction_Code\": code,\n",
        "                \"Building\": st.get(\"Building\", None),\n",
        "                \"BuildingName\": st.get(\"BuildingName\", None),\n",
        "                \"LocationName\": st.get(\"LocationName\", None),\n",
        "                \"ShortDescription\": f\"{kpi_row['KPIDescription']} period-crossing\",\n",
        "                \"LongDescription\": \"Intentionally spans rail period boundary\",\n",
        "                \"Reporter\": \"Ops Bot\",\n",
        "                \"ReporterEmail\": \"ops@example.com\",\n",
        "                \"Notes\": \"period-cross\",\n",
        "                \"ReportedDate\": reported,\n",
        "                \"DueBy\": reported + timedelta(days=3),\n",
        "                \"ScheduledFor\": reported,\n",
        "                \"Finished\": finish_time,\n",
        "                \"Status\": \"COMP\",\n",
        "                \"LoggedBy\": \"OpsConsole\",\n",
        "                \"LoggedOn\": reported,\n",
        "                \"ModifiedOn\": finish_time,\n",
        "                \"SLAStatus\": random.choice([\"MET\", \"BREACH\", \"AT_RISK\"]),\n",
        "                \"CreatedTimestamp\": reported,\n",
        "                \"LastUploaded\": finish_time,\n",
        "                \"IsCurrent\": True,\n",
        "                \"Period\": dd.iloc[0][\"Period\"],\n",
        "                \"PeriodWeek\": int(dd.iloc[0][\"PeriodWeek\"]) if not pd.isna(dd.iloc[0][\"PeriodWeek\"]) else None,\n",
        "                \"PeriodYear\": int(dd.iloc[0][\"PeriodYear\"]) if not pd.isna(dd.iloc[0][\"PeriodYear\"]) else None,\n",
        "                \"StationSection\": st[\"StationSection\"],\n",
        "                \"KPIDescription\": kpi_row[\"KPIDescription\"],\n",
        "                \"KPICategory\": kpi_row[\"KPICategory\"],\n",
        "            })\n",
        "\n",
        "    # 4) Overlap clusters by station/time for selected KPIs\n",
        "    for code in selected_kpis:\n",
        "        for _ in range(CONFIG[\"overlap_clusters_per_kpi\"]):\n",
        "            st = station_dim.loc[random.choice(stations_idx)]\n",
        "            center = random_datetime_within_window(START_DATE, END_DATE)\n",
        "            rows.extend(generate_overlap_cluster(code, st, center))\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Trim to exact target if overshot\n",
        "    if len(df) > CONFIG[\"target_row_count\"]:\n",
        "        df = df.sample(CONFIG[\"target_row_count\"], random_state=CONFIG[\"seed\"]).reset_index(drop=True)\n",
        "\n",
        "    # Enforce dtypes and column order\n",
        "    for col, dtype in SCHEMA_COLUMNS:\n",
        "        if col not in df.columns:\n",
        "            df[col] = pd.Series([None] * len(df))\n",
        "        if dtype.startswith(\"datetime\"):\n",
        "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
        "        elif dtype == \"bool\":\n",
        "            df[col] = df[col].astype(\"boolean\").fillna(True)\n",
        "        elif dtype == \"Int64\":\n",
        "            df[col] = df[col].astype(\"Int64\")\n",
        "        else:\n",
        "            df[col] = df[col].astype(\"string\")\n",
        "\n",
        "    df = df[[c for c, _ in SCHEMA_COLUMNS]]\n",
        "    return df\n",
        "\n",
        "\n",
        "df_tasks = generate_tasks()\n",
        "print(df_tasks.shape)\n",
        "df_tasks.head(3)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "1ca2bbe3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}