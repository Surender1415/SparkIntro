{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potential Failures Test Data Generator\n",
    "\n",
    "This notebook generates scalable test data for the app_potential_failures schema with configurable parameters.\n",
    "\n",
    "## Features:\n",
    "- Configurable KPI code selection (all or specific codes)\n",
    "- Variable task durations (short, medium, long) for each KPI group\n",
    "- Financial year spanning tasks with staggered dates\n",
    "- Period boundary crossing\n",
    "- Station distribution across all stations (excluding NULL sections)\n",
    "- Overlapping tasks for duplicate testing\n",
    "- Scalable record count (default ~15k)\n",
    "- All tasks in COMP status\n",
    "- Optional: Status progression simulation (WAPPR -> APPR -> COMP)\n",
    "- Optional: Non-KPI code tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import pyodbc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION FLAGS\n",
    "# ============================================================================\n",
    "\n",
    "CONFIG = {\n",
    "    # Data Generation Settings\n",
    "    'TOTAL_RECORDS': 15000,  # Total number of records to generate\n",
    "    'START_DATE': '2025-05-25',  # GTS started EL date\n",
    "    'END_DATE': '2027-05-25',  # Two year period\n",
    "    \n",
    "    # KPI Code Settings\n",
    "    'USE_ALL_KPI_CODES': True,  # Set to False to use specific codes\n",
    "    'SELECTED_KPI_CODES': [],  # List of specific KPI codes if USE_ALL_KPI_CODES is False\n",
    "    'INCLUDE_NON_KPI_TASKS': False,  # Nice-to-have: generate non-KPI tasks\n",
    "    'NON_KPI_TASK_PERCENTAGE': 0.05,  # 5% non-KPI tasks if enabled\n",
    "    \n",
    "    # Duration Settings (in hours)\n",
    "    'SHORT_DURATION_RANGE': (1, 24),  # Short tasks: 1-24 hours\n",
    "    'MEDIUM_DURATION_RANGE': (25, 168),  # Medium tasks: 1-7 days\n",
    "    'LONG_DURATION_RANGE': (169, 720),  # Long tasks: 7-30 days\n",
    "    'VERY_LONG_DURATION_RANGE': (721, 2160),  # Very long: 30-90 days for edge cases\n",
    "    \n",
    "    # Duration Distribution per KPI Code Group\n",
    "    'DURATION_DISTRIBUTION': {\n",
    "        'short': 0.40,  # 40% short duration\n",
    "        'medium': 0.35,  # 35% medium duration\n",
    "        'long': 0.20,  # 20% long duration\n",
    "        'very_long': 0.05  # 5% very long duration\n",
    "    },\n",
    "    \n",
    "    # Financial Year Settings\n",
    "    'FINANCIAL_YEAR_END': '03-31',  # MM-DD format (March 31st)\n",
    "    'ENSURE_FY_SPANNING_TASKS': True,  # 1 of each KPI code spans FY\n",
    "    'STAGGERED_ROLLOVER_TASKS': True,  # Tasks that test threshold rollover\n",
    "    \n",
    "    # Duplicate Testing Settings\n",
    "    'DUPLICATE_TEST_PERCENTAGE': 0.10,  # 10% of tasks will have close overlaps\n",
    "    'DUPLICATE_TIME_WINDOW_HOURS': 4,  # Within 4 hours for duplicate testing\n",
    "    \n",
    "    # Station Distribution\n",
    "    'EXCLUDE_NULL_SECTIONS': True,  # Exclude depots/NULL sections\n",
    "    \n",
    "    # Output Settings\n",
    "    'OUTPUT_TO_LH': True,  # Output to Lakehouse for validation\n",
    "    'LH_TABLE_NAME': 'test_potential_failures_validation',\n",
    "    'OUTPUT_TO_SQL': False,  # Set to True to write to SQL Server after validation\n",
    "    'SQL_TABLE_NAME': 'app_potential_failures_test',\n",
    "    \n",
    "    # Status Progression Simulation (Nice-to-have)\n",
    "    'SIMULATE_STATUS_PROGRESSION': False,  # Create multiple snapshots with status changes\n",
    "    'STATUS_PROGRESSION_STEPS': ['WAPPR', 'APPR', 'COMP'],  # Status progression\n",
    "    \n",
    "    # Task Frequency Weights by KPI Code (can be customized)\n",
    "    'KPI_FREQUENCY_WEIGHTS': {},  # Empty dict means equal distribution\n",
    "    # Example: {'GRAFFITI': 2.0, 'TRACKSIDE_CLEAN': 1.5, 'OTHER': 1.0}\n",
    "    \n",
    "    # Database Connection Settings\n",
    "    'SQL_SERVER': 'your_server.database.windows.net',\n",
    "    'SQL_DATABASE': 'your_database',\n",
    "    'SQL_USERNAME': 'your_username',\n",
    "    'SQL_PASSWORD': 'your_password',  # Use Azure Key Vault in production\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded successfully!\")\n",
    "print(f\"Target records: {CONFIG['TOTAL_RECORDS']}\")\n",
    "print(f\"Date range: {CONFIG['START_DATE']} to {CONFIG['END_DATE']}\")\n",
    "print(f\"Use all KPI codes: {CONFIG['USE_ALL_KPI_CODES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize Spark Session for Fabric/Databricks\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PotentialFailuresTestDataGenerator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# LOAD REFERENCE DATA FROM TABLES\n",
    "# ============================================================================\n",
    "\n",
    "# Load KPI Classification data\n",
    "print(\"Loading KPI classification data...\")\n",
    "kpi_classification_query = \"\"\"\n",
    "SELECT DISTINCT \n",
    "    KPICode,\n",
    "    KPIDescription,\n",
    "    KPICategory,\n",
    "    AnnualThresholdHours\n",
    "FROM bronze.fms_dimkpiclassification\n",
    "WHERE KPICode IS NOT NULL\n",
    "    AND IsKPI = 1  -- Only KPI codes\n",
    "ORDER BY KPICode\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    # Try to read from SQL (Fabric environment)\n",
    "    df_kpi = spark.read \\\n",
    "        .format(\"sqlserver\") \\\n",
    "        .option(\"query\", kpi_classification_query) \\\n",
    "        .load()\n",
    "    kpi_codes_df = df_kpi.toPandas()\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load from SQL. Using sample KPI codes. Error: {e}\")\n",
    "    # Sample KPI codes for testing\n",
    "    kpi_codes_df = pd.DataFrame({\n",
    "        'KPICode': ['GRAFFITI', 'TRACKSIDE_CLEAN', 'ESCALATOR_REPAIR', 'LIFT_MAINTENANCE', \n",
    "                    'STATION_CLEAN', 'PLATFORM_REPAIR', 'LIGHTING_FIX', 'SIGNAGE_UPDATE'],\n",
    "        'KPIDescription': ['Graffiti Removal', 'Trackside Cleaning', 'Escalator Repair', \n",
    "                          'Lift Maintenance', 'Station Cleaning', 'Platform Repair',\n",
    "                          'Lighting Fix', 'Signage Update'],\n",
    "        'KPICategory': ['Cleaning', 'Cleaning', 'Maintenance', 'Maintenance',\n",
    "                       'Cleaning', 'Maintenance', 'Maintenance', 'Maintenance'],\n",
    "        'AnnualThresholdHours': [24, 48, 100, 100, 48, 72, 24, 24]\n",
    "    })\n",
    "\n",
    "print(f\"Loaded {len(kpi_codes_df)} KPI codes\")\n",
    "\n",
    "# Filter KPI codes based on configuration\n",
    "if not CONFIG['USE_ALL_KPI_CODES'] and CONFIG['SELECTED_KPI_CODES']:\n",
    "    kpi_codes_df = kpi_codes_df[kpi_codes_df['KPICode'].isin(CONFIG['SELECTED_KPI_CODES'])]\n",
    "    print(f\"Using {len(kpi_codes_df)} selected KPI codes\")\n",
    "\n",
    "kpi_codes = kpi_codes_df.to_dict('records')\n",
    "print(\"Sample KPI codes:\", [k['KPICode'] for k in kpi_codes[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Station data\n",
    "print(\"\\nLoading station data...\")\n",
    "station_query = \"\"\"\n",
    "SELECT DISTINCT\n",
    "    StationCode,\n",
    "    StationName,\n",
    "    Section\n",
    "FROM customer_success.dimStation\n",
    "WHERE StationCode IS NOT NULL\n",
    "ORDER BY StationCode\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_stations = spark.read \\\n",
    "        .format(\"sqlserver\") \\\n",
    "        .option(\"query\", station_query) \\\n",
    "        .load()\n",
    "    stations_df = df_stations.toPandas()\n",
    "    \n",
    "    # Exclude NULL sections (depots) if configured\n",
    "    if CONFIG['EXCLUDE_NULL_SECTIONS']:\n",
    "        stations_df = stations_df[stations_df['Section'].notna()]\n",
    "        print(f\"Excluded stations with NULL sections (depots)\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load from SQL. Using sample stations. Error: {e}\")\n",
    "    # Sample stations for testing\n",
    "    stations_df = pd.DataFrame({\n",
    "        'StationCode': ['KGX', 'SPX', 'LBG', 'VIC', 'WAT', 'EUS', 'PAD', 'LST'],\n",
    "        'StationName': ['Kings Cross', 'St Pancras', 'London Bridge', 'Victoria',\n",
    "                       'Waterloo', 'Euston', 'Paddington', 'Liverpool Street'],\n",
    "        'Section': ['North', 'North', 'South', 'South', 'South', 'North', 'West', 'East']\n",
    "    })\n",
    "\n",
    "stations = stations_df.to_dict('records')\n",
    "print(f\"Loaded {len(stations)} stations\")\n",
    "print(\"Sample stations:\", [s['StationCode'] for s in stations[:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load Date Dimension data (for rail period and period week)\n",
    "print(\"\\nLoading date dimension data...\")\n",
    "date_query = f\"\"\"\n",
    "SELECT \n",
    "    Date,\n",
    "    RailPeriod,\n",
    "    RailPeriodWeek,\n",
    "    FiscalYear\n",
    "FROM core_dimdate\n",
    "WHERE Date >= '{CONFIG['START_DATE']}' \n",
    "    AND Date <= '{CONFIG['END_DATE']}'\n",
    "ORDER BY Date\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    df_dates = spark.read \\\n",
    "        .format(\"sqlserver\") \\\n",
    "        .option(\"query\", date_query) \\\n",
    "        .load()\n",
    "    dates_df = df_dates.toPandas()\n",
    "    dates_df['Date'] = pd.to_datetime(dates_df['Date'])\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not load from SQL. Generating sample date data. Error: {e}\")\n",
    "    # Generate sample date data\n",
    "    date_range = pd.date_range(start=CONFIG['START_DATE'], end=CONFIG['END_DATE'], freq='D')\n",
    "    dates_df = pd.DataFrame({\n",
    "        'Date': date_range,\n",
    "        'RailPeriod': [(d.year - 2025) * 13 + ((d.month - 1) // 4) + 1 for d in date_range],\n",
    "        'RailPeriodWeek': [((d - date_range[0]).days // 7) + 1 for d in date_range],\n",
    "        'FiscalYear': [d.year if d.month >= 4 else d.year - 1 for d in date_range]\n",
    "    })\n",
    "\n",
    "# Create lookup dictionary for dates\n",
    "date_lookup = dates_df.set_index('Date').to_dict('index')\n",
    "print(f\"Loaded {len(dates_df)} dates\")\n",
    "print(f\"Date range: {dates_df['Date'].min()} to {dates_df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_random_datetime(start_date: str, end_date: str) -> datetime:\n",
    "    \"\"\"Generate a random datetime between start and end dates.\"\"\"\n",
    "    start = pd.to_datetime(start_date)\n",
    "    end = pd.to_datetime(end_date)\n",
    "    delta = end - start\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    random_seconds = random.randint(0, 86400)  # Random time of day\n",
    "    return start + timedelta(days=random_days, seconds=random_seconds)\n",
    "\n",
    "def get_duration_category(distribution: Dict[str, float]) -> str:\n",
    "    \"\"\"Randomly select a duration category based on distribution weights.\"\"\"\n",
    "    categories = list(distribution.keys())\n",
    "    weights = list(distribution.values())\n",
    "    return random.choices(categories, weights=weights)[0]\n",
    "\n",
    "def get_duration_hours(category: str) -> int:\n",
    "    \"\"\"Get random duration in hours based on category.\"\"\"\n",
    "    ranges = {\n",
    "        'short': CONFIG['SHORT_DURATION_RANGE'],\n",
    "        'medium': CONFIG['MEDIUM_DURATION_RANGE'],\n",
    "        'long': CONFIG['LONG_DURATION_RANGE'],\n",
    "        'very_long': CONFIG['VERY_LONG_DURATION_RANGE']\n",
    "    }\n",
    "    min_h, max_h = ranges[category]\n",
    "    return random.randint(min_h, max_h)\n",
    "\n",
    "def get_period_info(date: datetime) -> Tuple[str, int, int]:\n",
    "    \"\"\"Get period information for a given date.\"\"\"\n",
    "    date_str = pd.to_datetime(date).strftime('%Y-%m-%d')\n",
    "    date_key = pd.to_datetime(date_str)\n",
    "    \n",
    "    if date_key in date_lookup:\n",
    "        info = date_lookup[date_key]\n",
    "        period = f\"P{info.get('RailPeriod', 1):02d}\"\n",
    "        period_week = info.get('RailPeriodWeek', 1)\n",
    "        period_year = info.get('FiscalYear', date.year)\n",
    "    else:\n",
    "        # Fallback if date not in lookup\n",
    "        period = f\"P{((date.month - 1) // 4) + 1:02d}\"\n",
    "        period_week = ((date - pd.to_datetime(CONFIG['START_DATE'])).days // 7) + 1\n",
    "        period_year = date.year if date.month >= 4 else date.year - 1\n",
    "    \n",
    "    return period, period_week, period_year\n",
    "\n",
    "def generate_task_id() -> str:\n",
    "    \"\"\"Generate a unique task ID.\"\"\"\n",
    "    return f\"TSK-{random.randint(100000, 999999)}-{random.randint(1000, 9999)}\"\n",
    "\n",
    "def generate_record_id() -> str:\n",
    "    \"\"\"Generate a unique record ID.\"\"\"\n",
    "    return f\"REC-{random.randint(1000000, 9999999)}\"\n",
    "\n",
    "def get_financial_year_dates(year: int) -> Tuple[datetime, datetime]:\n",
    "    \"\"\"Get start and end dates for a financial year.\"\"\"\n",
    "    fy_end_month, fy_end_day = map(int, CONFIG['FINANCIAL_YEAR_END'].split('-'))\n",
    "    fy_start = datetime(year - 1, fy_end_month, fy_end_day) + timedelta(days=1)\n",
    "    fy_end = datetime(year, fy_end_month, fy_end_day)\n",
    "    return fy_start, fy_end\n",
    "\n",
    "def should_span_financial_year(kpi_code: str, generated_fy_tasks: set) -> bool:\n",
    "    \"\"\"Check if this KPI code should have a FY-spanning task.\"\"\"\n",
    "    if not CONFIG['ENSURE_FY_SPANNING_TASKS']:\n",
    "        return False\n",
    "    return kpi_code not in generated_fy_tasks\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Generation Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# TASK GENERATION LOGIC\n",
    "# ============================================================================\n",
    "\n",
    "def create_task(kpi_info: Dict, station: Dict, \n",
    "                duration_category: Optional[str] = None,\n",
    "                force_fy_span: bool = False,\n",
    "                force_period_cross: bool = False,\n",
    "                reference_task: Optional[Dict] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Create a single task record.\n",
    "    \n",
    "    Args:\n",
    "        kpi_info: KPI code information\n",
    "        station: Station information\n",
    "        duration_category: Force specific duration category\n",
    "        force_fy_span: Force task to span financial year\n",
    "        force_period_cross: Force task to cross period boundary\n",
    "        reference_task: Reference task for creating overlapping duplicates\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate task identifiers\n",
    "    task_id = generate_task_id()\n",
    "    record_id = generate_record_id()\n",
    "    \n",
    "    # Determine duration\n",
    "    if duration_category is None:\n",
    "        duration_category = get_duration_category(CONFIG['DURATION_DISTRIBUTION'])\n",
    "    duration_hours = get_duration_hours(duration_category)\n",
    "    \n",
    "    # Generate dates\n",
    "    if reference_task:\n",
    "        # Create overlapping task for duplicate testing\n",
    "        ref_logged = reference_task['LoggedOn']\n",
    "        time_offset = random.randint(-CONFIG['DUPLICATE_TIME_WINDOW_HOURS'], \n",
    "                                     CONFIG['DUPLICATE_TIME_WINDOW_HOURS'])\n",
    "        logged_on = ref_logged + timedelta(hours=time_offset)\n",
    "    elif force_fy_span:\n",
    "        # Create task that spans financial year\n",
    "        fy_year = 2026  # First FY in our range\n",
    "        fy_start, fy_end = get_financial_year_dates(fy_year)\n",
    "        # Start before FY end\n",
    "        days_before = random.randint(30, 90)\n",
    "        logged_on = fy_end - timedelta(days=days_before)\n",
    "        # Ensure it ends after FY end\n",
    "        duration_hours = max(duration_hours, (days_before + 30) * 24)\n",
    "    else:\n",
    "        logged_on = get_random_datetime(CONFIG['START_DATE'], CONFIG['END_DATE'])\n",
    "    \n",
    "    # Calculate other dates\n",
    "    reported_date = logged_on - timedelta(hours=random.randint(1, 48))\n",
    "    scheduled_for = logged_on + timedelta(hours=random.randint(1, 24))\n",
    "    finished = logged_on + timedelta(hours=duration_hours)\n",
    "    \n",
    "    # Ensure finished date is within our range\n",
    "    end_date = pd.to_datetime(CONFIG['END_DATE'])\n",
    "    if finished > end_date:\n",
    "        finished = end_date - timedelta(hours=random.randint(1, 168))\n",
    "    \n",
    "    due_by = scheduled_for + timedelta(hours=random.randint(24, 168))\n",
    "    modified_on = finished + timedelta(hours=random.randint(1, 4))\n",
    "    \n",
    "    # Get period information based on logged date\n",
    "    period, period_week, period_year = get_period_info(logged_on)\n",
    "    \n",
    "    # If force_period_cross, adjust to cross period boundary\n",
    "    if force_period_cross:\n",
    "        # Find next period start\n",
    "        current_period = int(period.replace('P', ''))\n",
    "        # Adjust logged_on to be near period boundary\n",
    "        # This is simplified - in production, use actual period boundaries\n",
    "        pass\n",
    "    \n",
    "    # Sample data for various fields\n",
    "    reporters = ['John Smith', 'Jane Doe', 'Bob Wilson', 'Alice Brown', 'Charlie Davis']\n",
    "    reporter = random.choice(reporters)\n",
    "    reporter_email = f\"{reporter.lower().replace(' ', '.')}@rail.com\"\n",
    "    \n",
    "    instruction_codes = ['MAINT', 'CLEAN', 'REPAIR', 'INSPECT', 'REPLACE']\n",
    "    \n",
    "    # Create task record\n",
    "    task = {\n",
    "        'TaskId': task_id,\n",
    "        'RecordID': record_id,\n",
    "        'Instruction_Code': random.choice(instruction_codes),\n",
    "        'Building': station['StationCode'],\n",
    "        'BuildingName': station['StationName'],\n",
    "        'LocationName': f\"{station['StationName']} - {random.choice(['Platform', 'Concourse', 'Entrance', 'Office'])}\",\n",
    "        'ShortDescription': f\"{kpi_info['KPIDescription']} at {station['StationName']}\",\n",
    "        'LongDescription': f\"Complete {kpi_info['KPIDescription'].lower()} work at {station['StationName']}. Duration: {duration_hours} hours.\",\n",
    "        'Reporter': reporter,\n",
    "        'ReporterEmail': reporter_email,\n",
    "        'Notes': f\"Task completed. Duration category: {duration_category}\",\n",
    "        'ReportedDate': reported_date,\n",
    "        'DueBy': due_by,\n",
    "        'ScheduledFor': scheduled_for,\n",
    "        'Finished': finished,\n",
    "        'Status': 'COMP',  # All tasks are completed\n",
    "        'LoggedBy': f\"System-{random.randint(1, 10)}\",\n",
    "        'LoggedOn': logged_on,\n",
    "        'ModifiedOn': modified_on,\n",
    "        'SLAStatus': random.choice(['Met', 'Met', 'Met', 'Missed']),  # 75% met\n",
    "        'CreatedTimestamp': logged_on,\n",
    "        'LastUploaded': modified_on + timedelta(hours=1),\n",
    "        'IsCurrent': 1,  # bit field\n",
    "        'Period': period,\n",
    "        'PeriodWeek': period_week,\n",
    "        'PeriodYear': period_year,\n",
    "        'StationSection': station.get('Section', 'Unknown'),\n",
    "        'KPIDescription': kpi_info['KPIDescription'],\n",
    "        'KPICategory': kpi_info['KPICategory']\n",
    "    }\n",
    "    \n",
    "    return task\n",
    "\n",
    "print(\"Task generation function loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# GENERATE ALL TASKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Starting test data generation...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate distribution\n",
    "total_records = CONFIG['TOTAL_RECORDS']\n",
    "num_kpi_codes = len(kpi_codes)\n",
    "\n",
    "# Apply frequency weights if configured\n",
    "if CONFIG['KPI_FREQUENCY_WEIGHTS']:\n",
    "    weights = [CONFIG['KPI_FREQUENCY_WEIGHTS'].get(k['KPICode'], 1.0) for k in kpi_codes]\n",
    "else:\n",
    "    weights = [1.0] * num_kpi_codes\n",
    "\n",
    "# Normalize weights\n",
    "total_weight = sum(weights)\n",
    "normalized_weights = [w / total_weight for w in weights]\n",
    "\n",
    "# Calculate tasks per KPI code\n",
    "tasks_per_kpi = [int(total_records * w) for w in normalized_weights]\n",
    "\n",
    "# Adjust for rounding\n",
    "while sum(tasks_per_kpi) < total_records:\n",
    "    tasks_per_kpi[random.randint(0, len(tasks_per_kpi) - 1)] += 1\n",
    "\n",
    "print(f\"Generating {total_records} tasks across {num_kpi_codes} KPI codes\")\n",
    "print(f\"Average tasks per KPI code: {total_records / num_kpi_codes:.0f}\")\n",
    "print(f\"Stations available: {len(stations)}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Track generated tasks\n",
    "all_tasks = []\n",
    "fy_spanning_tasks = set()  # Track which KPI codes have FY-spanning tasks\n",
    "tasks_by_station = {}  # For duplicate testing\n",
    "\n",
    "# Generate tasks for each KPI code\n",
    "for kpi_idx, kpi_info in enumerate(kpi_codes):\n",
    "    kpi_code = kpi_info['KPICode']\n",
    "    num_tasks = tasks_per_kpi[kpi_idx]\n",
    "    \n",
    "    print(f\"\\nGenerating {num_tasks} tasks for {kpi_code}...\")\n",
    "    \n",
    "    # Ensure at least one of each duration category\n",
    "    duration_categories = list(CONFIG['DURATION_DISTRIBUTION'].keys())\n",
    "    tasks_generated = 0\n",
    "    \n",
    "    # 1. Generate FY-spanning task (if configured)\n",
    "    if CONFIG['ENSURE_FY_SPANNING_TASKS'] and kpi_code not in fy_spanning_tasks:\n",
    "        station = random.choice(stations)\n",
    "        task = create_task(kpi_info, station, force_fy_span=True)\n",
    "        all_tasks.append(task)\n",
    "        fy_spanning_tasks.add(kpi_code)\n",
    "        tasks_generated += 1\n",
    "    \n",
    "    # 2. Generate at least one task of each duration category\n",
    "    for duration_cat in duration_categories:\n",
    "        if tasks_generated >= num_tasks:\n",
    "            break\n",
    "        station = random.choice(stations)\n",
    "        task = create_task(kpi_info, station, duration_category=duration_cat)\n",
    "        all_tasks.append(task)\n",
    "        tasks_generated += 1\n",
    "    \n",
    "    # 3. Generate remaining tasks with random distribution\n",
    "    remaining_tasks = num_tasks - tasks_generated\n",
    "    \n",
    "    # Calculate how many should be duplicates\n",
    "    num_duplicates = int(remaining_tasks * CONFIG['DUPLICATE_TEST_PERCENTAGE'])\n",
    "    num_regular = remaining_tasks - num_duplicates\n",
    "    \n",
    "    # Generate regular tasks\n",
    "    for _ in range(num_regular):\n",
    "        station = random.choice(stations)\n",
    "        task = create_task(kpi_info, station)\n",
    "        all_tasks.append(task)\n",
    "        \n",
    "        # Store for potential duplicate creation\n",
    "        station_code = station['StationCode']\n",
    "        if station_code not in tasks_by_station:\n",
    "            tasks_by_station[station_code] = []\n",
    "        tasks_by_station[station_code].append(task)\n",
    "    \n",
    "    # Generate duplicate test tasks (overlapping times at same station)\n",
    "    for _ in range(num_duplicates):\n",
    "        # Pick a station that has existing tasks\n",
    "        available_stations = [s for s in tasks_by_station.keys() if tasks_by_station[s]]\n",
    "        if available_stations:\n",
    "            station_code = random.choice(available_stations)\n",
    "            reference_task = random.choice(tasks_by_station[station_code])\n",
    "            station = next(s for s in stations if s['StationCode'] == station_code)\n",
    "            task = create_task(kpi_info, station, reference_task=reference_task)\n",
    "            all_tasks.append(task)\n",
    "        else:\n",
    "            # Fallback to regular task\n",
    "            station = random.choice(stations)\n",
    "            task = create_task(kpi_info, station)\n",
    "            all_tasks.append(task)\n",
    "    \n",
    "    print(f\"  ✓ Generated {num_tasks} tasks for {kpi_code}\")\n",
    "    print(f\"    - FY spanning: {'Yes' if kpi_code in fy_spanning_tasks else 'No'}\")\n",
    "    print(f\"    - Duplicate test tasks: {num_duplicates}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Total tasks generated: {len(all_tasks)}\")\n",
    "print(f\"FY-spanning tasks: {len(fy_spanning_tasks)}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataFrame and Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# CREATE DATAFRAME AND VALIDATE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Creating DataFrame...\")\n",
    "df = pd.DataFrame(all_tasks)\n",
    "\n",
    "# Ensure correct data types\n",
    "datetime_columns = ['ReportedDate', 'DueBy', 'ScheduledFor', 'Finished', \n",
    "                   'LoggedOn', 'ModifiedOn', 'CreatedTimestamp', 'LastUploaded']\n",
    "for col in datetime_columns:\n",
    "    df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "# Validation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal records: {len(df)}\")\n",
    "print(f\"Date range: {df['LoggedOn'].min()} to {df['Finished'].max()}\")\n",
    "print(f\"\\nKPI codes distribution:\")\n",
    "print(df['KPIDescription'].value_counts())\n",
    "\n",
    "print(f\"\\nStation distribution (top 10):\")\n",
    "print(df['Building'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\nPeriod distribution:\")\n",
    "print(df['Period'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nStatus check:\")\n",
    "print(df['Status'].value_counts())\n",
    "\n",
    "# Check for FY-spanning tasks\n",
    "df['Duration_Days'] = (df['Finished'] - df['LoggedOn']).dt.total_seconds() / (24 * 3600)\n",
    "print(f\"\\nDuration statistics:\")\n",
    "print(df['Duration_Days'].describe())\n",
    "\n",
    "# Check for period crossings\n",
    "df['Finished_Period'] = df['Finished'].apply(lambda x: get_period_info(x)[0])\n",
    "period_crossings = (df['Period'] != df['Finished_Period']).sum()\n",
    "print(f\"\\nTasks crossing period boundaries: {period_crossings} ({period_crossings/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Check for potential duplicates\n",
    "df['LoggedOn_Hour'] = df['LoggedOn'].dt.floor('H')\n",
    "duplicates = df.groupby(['Building', 'LoggedOn_Hour']).size()\n",
    "duplicate_stations = (duplicates > 1).sum()\n",
    "print(f\"\\nStation-time combinations with multiple tasks (for duplicate testing): {duplicate_stations}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE RECORDS\")\n",
    "print(\"=\" * 80)\n",
    "print(df.head(10).to_string())\n",
    "\n",
    "# Clean up temporary columns\n",
    "df = df.drop(['Finished_Period', 'LoggedOn_Hour', 'Duration_Days'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to Lakehouse (Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# EXPORT TO LAKEHOUSE FOR VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "if CONFIG['OUTPUT_TO_LH']:\n",
    "    print(\"\\nWriting to Lakehouse for validation...\")\n",
    "    \n",
    "    # Convert to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    \n",
    "    # Write to Lakehouse\n",
    "    table_name = CONFIG['LH_TABLE_NAME']\n",
    "    \n",
    "    try:\n",
    "        spark_df.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .format(\"delta\") \\\n",
    "            .saveAsTable(table_name)\n",
    "        \n",
    "        print(f\"✓ Successfully wrote {len(df)} records to Lakehouse table: {table_name}\")\n",
    "        print(f\"\\nValidation query:\")\n",
    "        print(f\"SELECT * FROM {table_name} LIMIT 100\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error writing to Lakehouse: {e}\")\n",
    "        print(f\"Saving to local CSV instead...\")\n",
    "        df.to_csv(f\"{table_name}.csv\", index=False)\n",
    "        print(f\"✓ Saved to {table_name}.csv\")\n",
    "else:\n",
    "    print(\"\\nLakehouse output disabled. Saving to CSV...\")\n",
    "    df.to_csv(\"potential_failures_test_data.csv\", index=False)\n",
    "    print(\"✓ Saved to potential_failures_test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Export to SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# EXPORT TO SQL SERVER (After Validation)\n",
    "# ============================================================================\n",
    "\n",
    "if CONFIG['OUTPUT_TO_SQL']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXPORTING TO SQL SERVER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Create connection string\n",
    "        conn_str = (\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={CONFIG['SQL_SERVER']};\"\n",
    "            f\"DATABASE={CONFIG['SQL_DATABASE']};\"\n",
    "            f\"UID={CONFIG['SQL_USERNAME']};\"\n",
    "            f\"PWD={CONFIG['SQL_PASSWORD']}\"\n",
    "        )\n",
    "        \n",
    "        # Connect\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Create table if not exists\n",
    "        create_table_sql = f\"\"\"\n",
    "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = '{CONFIG['SQL_TABLE_NAME']}')\n",
    "        CREATE TABLE {CONFIG['SQL_TABLE_NAME']} (\n",
    "            TaskId nvarchar(255),\n",
    "            RecordID nvarchar(255),\n",
    "            Instruction_Code nvarchar(50),\n",
    "            Building nvarchar(100),\n",
    "            BuildingName nvarchar(255),\n",
    "            LocationName nvarchar(255),\n",
    "            ShortDescription nvarchar(500),\n",
    "            LongDescription nvarchar(max),\n",
    "            Reporter nvarchar(255),\n",
    "            ReporterEmail nvarchar(255),\n",
    "            Notes nvarchar(max),\n",
    "            ReportedDate datetime2,\n",
    "            DueBy datetime2,\n",
    "            ScheduledFor datetime2,\n",
    "            Finished datetime2,\n",
    "            Status nvarchar(50),\n",
    "            LoggedBy nvarchar(255),\n",
    "            LoggedOn datetime2,\n",
    "            ModifiedOn datetime2,\n",
    "            SLAStatus nvarchar(50),\n",
    "            CreatedTimestamp datetime2,\n",
    "            LastUploaded datetime2,\n",
    "            IsCurrent bit,\n",
    "            Period nvarchar(10),\n",
    "            PeriodWeek bigint,\n",
    "            PeriodYear bigint,\n",
    "            StationSection nvarchar(100),\n",
    "            KPIDescription nvarchar(255),\n",
    "            KPICategory nvarchar(100)\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(create_table_sql)\n",
    "        conn.commit()\n",
    "        \n",
    "        print(f\"✓ Table {CONFIG['SQL_TABLE_NAME']} ready\")\n",
    "        \n",
    "        # Insert data in batches\n",
    "        batch_size = 1000\n",
    "        total_batches = (len(df) + batch_size - 1) // batch_size\n",
    "        \n",
    "        print(f\"\\nInserting {len(df)} records in {total_batches} batches...\")\n",
    "        \n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i+batch_size]\n",
    "            \n",
    "            # Prepare insert statement\n",
    "            insert_sql = f\"\"\"\n",
    "            INSERT INTO {CONFIG['SQL_TABLE_NAME']} \n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute batch insert\n",
    "            cursor.executemany(insert_sql, batch.values.tolist())\n",
    "            conn.commit()\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"  Batch {i // batch_size + 1}/{total_batches} completed\")\n",
    "        \n",
    "        print(f\"\\n✓ Successfully inserted {len(df)} records into {CONFIG['SQL_TABLE_NAME']}\")\n",
    "        \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error writing to SQL Server: {e}\")\n",
    "        print(\"Data is still available in Lakehouse for validation.\")\n",
    "else:\n",
    "    print(\"\\nSQL Server export disabled.\")\n",
    "    print(\"Set CONFIG['OUTPUT_TO_SQL'] = True to enable after validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Generate Status Progression Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: GENERATE STATUS PROGRESSION FILES\n",
    "# ============================================================================\n",
    "\n",
    "if CONFIG['SIMULATE_STATUS_PROGRESSION']:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATING STATUS PROGRESSION FILES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    status_steps = CONFIG['STATUS_PROGRESSION_STEPS']\n",
    "    \n",
    "    for step_idx, status in enumerate(status_steps):\n",
    "        print(f\"\\nGenerating snapshot {step_idx + 1}: {status} status\")\n",
    "        \n",
    "        # Create a copy of the dataframe\n",
    "        df_snapshot = df.copy()\n",
    "        \n",
    "        # Update status\n",
    "        df_snapshot['Status'] = status\n",
    "        \n",
    "        # Adjust Finished date based on status\n",
    "        if status != 'COMP':\n",
    "            df_snapshot['Finished'] = pd.NaT  # Not finished yet\n",
    "        \n",
    "        # Adjust ModifiedOn to simulate progression\n",
    "        time_offset = timedelta(hours=step_idx * 24)  # Each step is a day apart\n",
    "        df_snapshot['ModifiedOn'] = df_snapshot['LoggedOn'] + time_offset\n",
    "        \n",
    "        # Save to file\n",
    "        filename = f\"potential_failures_snapshot_{step_idx + 1}_{status}.csv\"\n",
    "        df_snapshot.to_csv(filename, index=False)\n",
    "        print(f\"  ✓ Saved {len(df_snapshot)} records to {filename}\")\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(status_steps)} status progression snapshots\")\n",
    "    print(\"These files can be used to test backload changes.\")\n",
    "else:\n",
    "    print(\"\\nStatus progression simulation disabled.\")\n",
    "    print(\"Set CONFIG['SIMULATE_STATUS_PROGRESSION'] = True to enable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# ============================================================================\n",
    "# SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"  - Total records generated: {len(df):,}\")\n",
    "print(f\"  - KPI codes covered: {df['KPIDescription'].nunique()}\")\n",
    "print(f\"  - Stations covered: {df['Building'].nunique()}\")\n",
    "print(f\"  - Date range: {df['LoggedOn'].min().date()} to {df['Finished'].max().date()}\")\n",
    "print(f\"  - Periods covered: {df['Period'].nunique()}\")\n",
    "print(f\"  - FY-spanning tasks: {len(fy_spanning_tasks)}\")\n",
    "\n",
    "print(f\"\\n📝 Next Steps:\")\n",
    "print(f\"  1. Validate data in Lakehouse table: {CONFIG['LH_TABLE_NAME']}\")\n",
    "print(f\"  2. Check for data quality issues\")\n",
    "print(f\"  3. Verify period crossings and FY spanning tasks\")\n",
    "print(f\"  4. Test duplicate detection logic\")\n",
    "print(f\"  5. Once validated, set CONFIG['OUTPUT_TO_SQL'] = True to export to SQL Server\")\n",
    "\n",
    "print(f\"\\n🔧 Configuration Tips:\")\n",
    "print(f\"  - Adjust CONFIG['TOTAL_RECORDS'] to scale up/down\")\n",
    "print(f\"  - Use CONFIG['SELECTED_KPI_CODES'] to focus on specific codes\")\n",
    "print(f\"  - Modify CONFIG['KPI_FREQUENCY_WEIGHTS'] to adjust distribution\")\n",
    "print(f\"  - Enable CONFIG['SIMULATE_STATUS_PROGRESSION'] for history testing\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✨ Test data generation completed successfully! ✨\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
