{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthetic Potential Failures Generator (Configurable)\n",
        "\n",
        "This notebook generates a scalable, configurable synthetic dataset matching the schema of `customer_success.app_potential_failures` with full control over KPI code selection, task frequencies, durations, overlapping windows, and period/financial year edge cases.\n",
        "\n",
        "Key capabilities:\n",
        "- Generate ~15k records (configurable) across a two-year window starting 2025-05-25 (GTS EL start).\n",
        "- Cover all KPI codes from `bronze.fms_dimkpiclassification` (or a selected subset via flags).\n",
        "- Ensure for every KPI code at least one job spans the financial year boundary and crosses rail period boundaries.\n",
        "- Create a realistic mix of short/medium/long/extra-long durations; include overlapping jobs to test duplicate tracking.\n",
        "- Distribute tasks across all stations from `customer_success.dimStation` (filtering out depot/NULL sections).\n",
        "- Join to `core_dimdate` to populate `Period`, `PeriodWeek`, and `PeriodYear`.\n",
        "- All jobs default to `COMP` status. Optional history mode can simulate WAPPR → APPR → COMP.\n",
        "- Write to Lakehouse first for validation, then optionally publish to SQL Server with identical schema.\n",
        "\n",
        "How to use:\n",
        "1) Configure flags below (source modes for dimensions: `jdbc`, `csv`, or `mock`).\n",
        "2) Run the notebook top-to-bottom to generate and validate the dataset in Lakehouse (local path).\n",
        "3) After validation, enable SQL write flags to publish to a SQL table with the same schema as `app_potential_failures`.\n",
        ""
      ],
      "id": "666dc5ae"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configuration & Imports\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import uuid\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Optional: SQLAlchemy / pyodbc for SQL Server writes\n",
        "try:\n",
        "    import sqlalchemy as sa\n",
        "except Exception:\n",
        "    sa = None\n",
        "\n",
        "# Optional: pyspark for Lakehouse writes in Fabric/Synapse if available\n",
        "try:\n",
        "    from pyspark.sql import SparkSession\n",
        "    from pyspark.sql import functions as F\n",
        "    from pyspark.sql import types as T\n",
        "except Exception:\n",
        "    SparkSession = None\n",
        "    F = None\n",
        "    T = None\n",
        "\n",
        "@dataclass\n",
        "class GeneratorConfig:\n",
        "    # record volume\n",
        "    total_records: int = 15000\n",
        "    # date range\n",
        "    start_date: datetime = datetime(2025, 5, 25)\n",
        "    end_date: datetime = datetime(2027, 5, 24)\n",
        "    # KPI code controls\n",
        "    include_all_kpi_codes: bool = True\n",
        "    selected_kpi_codes: Optional[List[str]] = None\n",
        "    # frequency per KPI code multiplier (1.0 baseline)\n",
        "    kpi_frequency_overrides: Optional[Dict[str, float]] = None\n",
        "    # overlapping windows control\n",
        "    enable_overlap_groups: bool = True\n",
        "    overlap_group_fraction: float = 0.10  # 10% have close overlaps per station/date\n",
        "    overlap_hours_window: int = 4\n",
        "    # duration mix per KPI\n",
        "    short_hours: Tuple[int, int] = (1, 4)\n",
        "    medium_hours: Tuple[int, int] = (6, 24)\n",
        "    long_hours: Tuple[int, int] = (24, 96)\n",
        "    extra_long_hours: Tuple[int, int] = (96, 240)  # up to 10 days\n",
        "    duration_mix_weights: Tuple[float, float, float, float] = (0.35, 0.35, 0.20, 0.10)\n",
        "    # ensure one FY-spanning job per KPI code\n",
        "    ensure_fy_spanners: bool = True\n",
        "    financial_year_end_month: int = 3  # March\n",
        "    financial_year_end_day: int = 31\n",
        "    # rail period join\n",
        "    require_period_join: bool = True\n",
        "    # dimensions sources: 'jdbc' | 'csv' | 'mock'\n",
        "    kpi_source_mode: str = 'mock'\n",
        "    stations_source_mode: str = 'mock'\n",
        "    dimdate_source_mode: str = 'mock'\n",
        "    # IO paths\n",
        "    local_output_dir: str = './lh_out'\n",
        "    local_dim_cache_dir: str = './lh_dims'\n",
        "    lakehouse_output_table: Optional[str] = None  # e.g., 'Tables/synthetic_app_potential_failures'\n",
        "    # SQL Server\n",
        "    enable_sql_write: bool = False\n",
        "    sql_connection_string: Optional[str] = None  # e.g., 'mssql+pyodbc://...'\n",
        "    sql_target_table: Optional[str] = None  # schema-qualified\n",
        "    # history mode\n",
        "    enable_history_mode: bool = False\n",
        "    history_fraction: float = 0.15\n",
        "    # allow generating non-KPI codes optionally\n",
        "    enable_non_kpi: bool = False\n",
        "    non_kpi_fraction: float = 0.05\n",
        "    # station filters\n",
        "    include_depots: bool = False\n",
        "\n",
        "config = GeneratorConfig()\n",
        "\n",
        "os.makedirs(config.local_output_dir, exist_ok=True)\n",
        "os.makedirs(config.local_dim_cache_dir, exist_ok=True)\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "442bb2c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helpers\n",
        "\n",
        "def random_string(n: int = 8) -> str:\n",
        "    return ''.join(random.choices(string.ascii_uppercase + string.digits, k=n))\n",
        "\n",
        "\n",
        "def random_email(name: str) -> str:\n",
        "    domain = random.choice([\"example.com\", \"railops.org\", \"cleanit.io\", \"facilities.net\"])\n",
        "    return f\"{name.lower()}@{domain}\"\n",
        "\n",
        "\n",
        "def choose_weighted(weights: Tuple[float, ...]) -> int:\n",
        "    return int(np.random.choice(len(weights), p=np.array(weights)/np.sum(weights)))\n",
        "\n",
        "\n",
        "def make_datetime_range(start: datetime, end: datetime) -> datetime:\n",
        "    if start > end:\n",
        "        start, end = end, start\n",
        "    span_seconds = int((end - start).total_seconds())\n",
        "    offset = random.randint(0, max(0, span_seconds - 1))\n",
        "    return start + timedelta(seconds=offset)\n",
        "\n",
        "\n",
        "def ensure_end_after(start: datetime, min_delta_hours: int, max_delta_hours: int) -> datetime:\n",
        "    hours = random.randint(min_delta_hours, max_delta_hours)\n",
        "    # randomize minutes within the hour window\n",
        "    minutes = random.randint(0, 59)\n",
        "    return start + timedelta(hours=hours, minutes=minutes)\n",
        "\n",
        "\n",
        "def compute_financial_year(date_val: datetime, fy_end_month: int, fy_end_day: int) -> int:\n",
        "    fy_end = datetime(date_val.year, fy_end_month, fy_end_day)\n",
        "    return date_val.year if date_val <= fy_end else date_val.year + 1\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "67394a90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mock/load dimensions: KPI codes, Stations, core_dimdate\n",
        "\n",
        "# In real mode (jdbc/csv), implement readers. For now, provide mock fallbacks.\n",
        "\n",
        "def load_kpi_codes(mode: str, selected: Optional[List[str]] = None) -> pd.DataFrame:\n",
        "    if mode == 'mock':\n",
        "        # Representative KPI codes including graffiti, track cleaning, and a few others\n",
        "        codes = [\n",
        "            (\"KPI_GRAFFITI\", \"Graffiti removal\", \"Cleanliness\"),\n",
        "            (\"KPI_TRACKCLEAN\", \"Trackside cleaning\", \"Cleanliness\"),\n",
        "            (\"KPI_LITTER\", \"Litter removal\", \"Cleanliness\"),\n",
        "            (\"KPI_LIGHT\", \"Lighting fault\", \"Asset\"),\n",
        "            (\"KPI_ESCALATOR\", \"Escalator fault\", \"Asset\"),\n",
        "            (\"KPI_SIGNAGE\", \"Signage damage\", \"Asset\"),\n",
        "            (\"KPI_VANDAL\", \"Vandalism repair\", \"Security\"),\n",
        "            (\"KPI_WATER\", \"Water leak\", \"Asset\"),\n",
        "            (\"KPI_ELECTRICAL\", \"Electrical fault\", \"Asset\"),\n",
        "            (\"KPI_TRACKOBST\", \"Track obstruction\", \"Operations\"),\n",
        "        ]\n",
        "        df = pd.DataFrame(codes, columns=[\"Instruction_Code\", \"KPIDescription\", \"KPICategory\"])\n",
        "    elif mode == 'csv':\n",
        "        df = pd.read_csv(os.path.join(config.local_dim_cache_dir, 'bronze.fms_dimkpiclassification.csv'))\n",
        "    else:  # jdbc placeholder\n",
        "        # Implement your JDBC reader here if running in Fabric/Spark environment\n",
        "        raise NotImplementedError(\"JDBC mode not implemented in this environment\")\n",
        "\n",
        "    if selected is not None and len(selected) > 0:\n",
        "        df = df[df['Instruction_Code'].isin(selected)].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_stations(mode: str, include_depots: bool = False) -> pd.DataFrame:\n",
        "    if mode == 'mock':\n",
        "        stations = [\n",
        "            (\"STN001\", \"Central\", \"Main Line\", \"Section A\"),\n",
        "            (\"STN002\", \"North\", \"North Line\", \"Section B\"),\n",
        "            (\"STN003\", \"East\", \"East Line\", \"Section C\"),\n",
        "            (\"STN004\", \"West\", \"West Line\", None),  # depot-like, should be dropped if include_depots False\n",
        "            (\"STN005\", \"South\", \"South Line\", \"Section D\"),\n",
        "        ]\n",
        "        df = pd.DataFrame(stations, columns=[\"StationCode\", \"StationName\", \"Line\", \"StationSection\"])\n",
        "    elif mode == 'csv':\n",
        "        df = pd.read_csv(os.path.join(config.local_dim_cache_dir, 'customer_success.dimStation.csv'))\n",
        "    else:\n",
        "        raise NotImplementedError(\"JDBC mode not implemented in this environment\")\n",
        "\n",
        "    if not include_depots:\n",
        "        df = df[df['StationSection'].notna()].reset_index(drop=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_dimdate(mode: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:\n",
        "    if mode == 'mock':\n",
        "        # Build a simple core_dimdate with rail periods (4-5 weeks cycles mocked)\n",
        "        dates = pd.date_range(start=start_date - timedelta(days=120), end=end_date + timedelta(days=120), freq='D')\n",
        "        period_years = []\n",
        "        periods = []\n",
        "        period_weeks = []\n",
        "        week_in_period = 0\n",
        "        period = 1\n",
        "        period_year = start_date.year\n",
        "        # naive rolling period model: 13 periods per year, 4 or 5 weeks alternation\n",
        "        pattern = [4,4,5]*4 + [4]\n",
        "        pat_idx = 0\n",
        "        day_idx = 0\n",
        "        for d in dates:\n",
        "            if day_idx % 7 == 0:  # new week\n",
        "                week_in_period += 1\n",
        "                if week_in_period > pattern[pat_idx % len(pattern)]:\n",
        "                    week_in_period = 1\n",
        "                    period += 1\n",
        "                    if period > 13:\n",
        "                        period = 1\n",
        "                        period_year += 1\n",
        "                    pat_idx += 1\n",
        "            period_weeks.append(week_in_period)\n",
        "            periods.append(period)\n",
        "            period_years.append(period_year)\n",
        "            day_idx += 1\n",
        "        df = pd.DataFrame({\n",
        "            'Date': dates,\n",
        "            'PeriodWeek': period_weeks,\n",
        "            'Period': periods,\n",
        "            'PeriodYear': period_years,\n",
        "        })\n",
        "    elif mode == 'csv':\n",
        "        df = pd.read_csv(os.path.join(config.local_dim_cache_dir, 'core_dimdate.csv'))\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "    else:\n",
        "        raise NotImplementedError(\"JDBC mode not implemented in this environment\")\n",
        "\n",
        "    return df\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "24e785d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generator\n",
        "\n",
        "SCHEMA_COLUMNS = [\n",
        "    (\"TaskId\", \"nvarchar\"),\n",
        "    (\"RecordID\", \"nvarchar\"),\n",
        "    (\"Instruction_Code\", \"nvarchar\"),\n",
        "    (\"Building\", \"nvarchar\"),\n",
        "    (\"BuildingName\", \"nvarchar\"),\n",
        "    (\"LocationName\", \"nvarchar\"),\n",
        "    (\"ShortDescription\", \"nvarchar\"),\n",
        "    (\"LongDescription\", \"nvarchar\"),\n",
        "    (\"Reporter\", \"nvarchar\"),\n",
        "    (\"ReporterEmail\", \"nvarchar\"),\n",
        "    (\"Notes\", \"nvarchar\"),\n",
        "    (\"ReportedDate\", \"datetime2\"),\n",
        "    (\"DueBy\", \"datetime2\"),\n",
        "    (\"ScheduledFor\", \"datetime2\"),\n",
        "    (\"Finished\", \"datetime2\"),\n",
        "    (\"Status\", \"nvarchar\"),\n",
        "    (\"LoggedBy\", \"nvarchar\"),\n",
        "    (\"LoggedOn\", \"datetime2\"),\n",
        "    (\"ModifiedOn\", \"datetime2\"),\n",
        "    (\"SLAStatus\", \"nvarchar\"),\n",
        "    (\"CreatedTimestamp\", \"datetime2\"),\n",
        "    (\"LastUploaded\", \"datetime2\"),\n",
        "    (\"IsCurrent\", \"bit\"),\n",
        "    (\"Period\", \"nvarchar\"),\n",
        "    (\"PeriodWeek\", \"bigint\"),\n",
        "    (\"PeriodYear\", \"bigint\"),\n",
        "    (\"StationSection\", \"nvarchar\"),\n",
        "    (\"KPIDescription\", \"nvarchar\"),\n",
        "    (\"KPICategory\", \"nvarchar\"),\n",
        "]\n",
        "\n",
        "\n",
        "def pick_duration_hours(mix: Tuple[float, float, float, float]) -> Tuple[int, int]:\n",
        "    bucket = choose_weighted(mix)\n",
        "    if bucket == 0:\n",
        "        return config.short_hours\n",
        "    if bucket == 1:\n",
        "        return config.medium_hours\n",
        "    if bucket == 2:\n",
        "        return config.long_hours\n",
        "    return config.extra_long_hours\n",
        "\n",
        "\n",
        "def generate_for_kpi(\n",
        "    kpi_row: pd.Series, stations: pd.DataFrame, dimdate: pd.DataFrame, num_records: int\n",
        ") -> pd.DataFrame:\n",
        "    records = []\n",
        "\n",
        "    kpi_code = kpi_row['Instruction_Code']\n",
        "    kpi_desc = kpi_row.get('KPIDescription', '')\n",
        "    kpi_cat = kpi_row.get('KPICategory', '')\n",
        "\n",
        "    # Preselect a proportion of overlapping jobs by station and approximate logged time\n",
        "    overlap_targets = set()\n",
        "    if config.enable_overlap_groups and num_records > 10:\n",
        "        desired_overlap = int(num_records * config.overlap_group_fraction)\n",
        "        for _ in range(desired_overlap):\n",
        "            stn = stations.sample(1).iloc[0]\n",
        "            base_time = make_datetime_range(config.start_date, config.end_date)\n",
        "            overlap_targets.add((stn['StationCode'], base_time.replace(minute=0, second=0, microsecond=0)))\n",
        "\n",
        "    # Ensure a FY-spanner for this KPI code if requested\n",
        "    fy_spanner_created = False\n",
        "    if config.ensure_fy_spanners:\n",
        "        # Choose a date close to FY end and span across\n",
        "        fy_end_this = datetime(config.start_date.year, config.financial_year_end_month, config.financial_year_end_day)\n",
        "        start_spanner = fy_end_this - timedelta(days=random.randint(1, 10))\n",
        "        end_spanner = fy_end_this + timedelta(days=random.randint(1, 15))\n",
        "        # Ensure falls within global window\n",
        "        start_spanner = max(config.start_date, start_spanner)\n",
        "        end_spanner = min(config.end_date, end_spanner)\n",
        "        stn = stations.sample(1).iloc[0]\n",
        "        records.append(\n",
        "            build_record(kpi_code, kpi_desc, kpi_cat, stn, start_spanner, end_spanner, status=\"COMP\", fy_spanner=True)\n",
        "        )\n",
        "        fy_spanner_created = True\n",
        "\n",
        "    for i in range(num_records - (1 if fy_spanner_created else 0)):\n",
        "        stn = stations.sample(1).iloc[0]\n",
        "\n",
        "        # LoggedOn as base, within the two-year window\n",
        "        logged_on = make_datetime_range(config.start_date, config.end_date)\n",
        "\n",
        "        # Overlap adjustment: if this station + hour bucket is tagged, cluster within overlap window\n",
        "        key = (stn['StationCode'], logged_on.replace(minute=0, second=0, microsecond=0))\n",
        "        if key in overlap_targets:\n",
        "            # shift within +/- overlap_hours_window\n",
        "            shift_hours = random.randint(-config.overlap_hours_window, config.overlap_hours_window)\n",
        "            logged_on = logged_on + timedelta(hours=shift_hours)\n",
        "\n",
        "        # Determine duration bucket and compute end\n",
        "        dmin, dmax = pick_duration_hours(config.duration_mix_weights)\n",
        "        finished = ensure_end_after(logged_on, dmin, dmax)\n",
        "\n",
        "        # DueBy and ScheduledFor around LoggedOn\n",
        "        due_by = logged_on + timedelta(hours=random.randint(4, 72))\n",
        "        scheduled_for = logged_on + timedelta(hours=random.randint(0, 24))\n",
        "\n",
        "        records.append(\n",
        "            build_record(kpi_code, kpi_desc, kpi_cat, stn, logged_on, finished, status=\"COMP\",\n",
        "                         due_by=due_by, scheduled_for=scheduled_for)\n",
        "        )\n",
        "\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Join to core_dimdate to add period fields based on LoggedOn date\n",
        "    if config.require_period_join and not df.empty:\n",
        "        dd = dimdate[[\"Date\", \"Period\", \"PeriodWeek\", \"PeriodYear\"]].copy()\n",
        "        dd['Date'] = pd.to_datetime(dd['Date']).dt.date\n",
        "        df['LoggedOnDate'] = pd.to_datetime(df['LoggedOn']).dt.date\n",
        "        df = df.merge(dd, left_on='LoggedOnDate', right_on='Date', how='left')\n",
        "        df.drop(columns=['Date', 'LoggedOnDate'], inplace=True)\n",
        "        # Ensure correct dtypes/names\n",
        "        df['Period'] = df['Period'].astype('Int64').astype(str)\n",
        "        df['PeriodWeek'] = df['PeriodWeek'].astype('Int64')\n",
        "        df['PeriodYear'] = df['PeriodYear'].astype('Int64')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def build_record(\n",
        "    kpi_code: str,\n",
        "    kpi_desc: str,\n",
        "    kpi_cat: str,\n",
        "    stn: pd.Series,\n",
        "    logged_on: datetime,\n",
        "    finished: datetime,\n",
        "    status: str = \"COMP\",\n",
        "    due_by: Optional[datetime] = None,\n",
        "    scheduled_for: Optional[datetime] = None,\n",
        "    fy_spanner: bool = False,\n",
        ") -> Dict[str, object]:\n",
        "    short_desc = f\"{kpi_desc or kpi_code} at {stn['StationName']}\"\n",
        "    long_desc = f\"{short_desc} — {random_string(12)}\"\n",
        "    reporter_name = random.choice([\"Alex\", \"Sam\", \"Jordan\", \"Casey\", \"Taylor\", \"Morgan\"]) + \" \" + random.choice([\"Lee\", \"Smith\", \"Patel\", \"Khan\", \"Brown\", \"Lopez\"]) \n",
        "    reporter_email = random_email(reporter_name.replace(\" \", \".\"))\n",
        "\n",
        "    # Model SLA status roughly\n",
        "    sla_status = random.choice([\"Within\", \"Breached\", \"At Risk\"]) if not fy_spanner else \"Within\"\n",
        "\n",
        "    row = {\n",
        "        \"TaskId\": str(uuid.uuid4()),\n",
        "        \"RecordID\": str(uuid.uuid4()),\n",
        "        \"Instruction_Code\": kpi_code,\n",
        "        \"Building\": stn['Line'],\n",
        "        \"BuildingName\": stn['StationName'],\n",
        "        \"LocationName\": stn['StationCode'],\n",
        "        \"ShortDescription\": short_desc,\n",
        "        \"LongDescription\": long_desc,\n",
        "        \"Reporter\": reporter_name,\n",
        "        \"ReporterEmail\": reporter_email,\n",
        "        \"Notes\": random.choice([\"\", \"Follow-up required\", \"High priority\", \"Photo attached\", \"Repeat issue\"]),\n",
        "        \"ReportedDate\": logged_on,\n",
        "        \"DueBy\": due_by or (logged_on + timedelta(hours=24)),\n",
        "        \"ScheduledFor\": scheduled_for or (logged_on + timedelta(hours=8)),\n",
        "        \"Finished\": finished,\n",
        "        \"Status\": status,\n",
        "        \"LoggedBy\": random.choice([\"FMS\", \"CRM\", \"MobileApp\", \"Email\"]),\n",
        "        \"LoggedOn\": logged_on,\n",
        "        \"ModifiedOn\": finished,\n",
        "        \"SLAStatus\": sla_status,\n",
        "        \"CreatedTimestamp\": logged_on,\n",
        "        \"LastUploaded\": finished,\n",
        "        \"IsCurrent\": True,\n",
        "        \"StationSection\": stn['StationSection'],\n",
        "        \"KPIDescription\": kpi_desc,\n",
        "        \"KPICategory\": kpi_cat,\n",
        "    }\n",
        "\n",
        "    return row\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "678f6d39"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main orchestration\n",
        "\n",
        "kpis = load_kpi_codes(config.kpi_source_mode, config.selected_kpi_codes)\n",
        "stations = load_stations(config.stations_source_mode, include_depots=config.include_depots)\n",
        "dimdate = load_dimdate(config.dimdate_source_mode, config.start_date, config.end_date)\n",
        "\n",
        "if config.include_all_kpi_codes:\n",
        "    active_kpis = kpis\n",
        "else:\n",
        "    if config.selected_kpi_codes:\n",
        "        active_kpis = kpis[kpis['Instruction_Code'].isin(config.selected_kpi_codes)]\n",
        "    else:\n",
        "        active_kpis = kpis.head(1)\n",
        "\n",
        "# Determine per-KPI record counts using frequency overrides\n",
        "base_per_kpi = max(1, int(config.total_records / max(1, len(active_kpis))))\n",
        "allocations = {}\n",
        "remaining = config.total_records\n",
        "for code in active_kpis['Instruction_Code']:\n",
        "    mult = (config.kpi_frequency_overrides or {}).get(code, 1.0)\n",
        "    count = max(1, int(base_per_kpi * mult))\n",
        "    allocations[code] = count\n",
        "    remaining -= count\n",
        "# Distribute any remainder\n",
        "codes_cycle = list(active_kpis['Instruction_Code'])\n",
        "idx = 0\n",
        "while remaining > 0 and codes_cycle:\n",
        "    allocations[codes_cycle[idx % len(codes_cycle)]] += 1\n",
        "    idx += 1\n",
        "    remaining -= 1\n",
        "\n",
        "frames = []\n",
        "for _, krow in active_kpis.iterrows():\n",
        "    count = allocations[krow['Instruction_Code']]\n",
        "    frames.append(generate_for_kpi(krow, stations, dimdate, count))\n",
        "\n",
        "result_df = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(columns=[c for c,_ in SCHEMA_COLUMNS])\n",
        "\n",
        "# Ensure schema columns exist and order\n",
        "for col, _ in SCHEMA_COLUMNS:\n",
        "    if col not in result_df.columns:\n",
        "        result_df[col] = np.nan\n",
        "result_df = result_df[[c for c,_ in SCHEMA_COLUMNS]]\n",
        "\n",
        "print(\"Generated records:\", len(result_df))\n",
        "result_df.head(3)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "6eb7d624"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validation & summary\n",
        "\n",
        "assert (result_df['Status'] == 'COMP').all(), \"All jobs should be COMP by default\"\n",
        "assert result_df['Instruction_Code'].notna().all(), \"All rows should have KPI codes\"\n",
        "\n",
        "by_kpi = result_df.groupby('Instruction_Code').size().sort_values(ascending=False)\n",
        "print(\"Counts by KPI code:\\n\", by_kpi)\n",
        "\n",
        "# Check rail period coverage\n",
        "period_counts = result_df.groupby(['PeriodYear', 'Period']).size()\n",
        "print(\"\\nCounts by PeriodYear/Period (sample):\\n\", period_counts.head(20))\n",
        "\n",
        "# Check durations histogram (Finished - LoggedOn)\n",
        "_durations_hours = (\n",
        "    (pd.to_datetime(result_df['Finished']) - pd.to_datetime(result_df['LoggedOn']))\n",
        "    .dt.total_seconds() / 3600.0\n",
        ")\n",
        "print(\"\\nDuration hours (summary):\\n\", _durations_hours.describe())"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d32b6996"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write to Lakehouse (local filesystem as stand-in) and optionally to SQL Server\n",
        "\n",
        "# Lakehouse/local: write parquet and csv\n",
        "parquet_path = os.path.join(config.local_output_dir, 'synthetic_app_potential_failures.parquet')\n",
        "csv_path = os.path.join(config.local_output_dir, 'synthetic_app_potential_failures.csv')\n",
        "\n",
        "result_df.to_parquet(parquet_path, index=False)\n",
        "result_df.to_csv(csv_path, index=False)\n",
        "print(f\"Wrote {len(result_df)} rows to {parquet_path} and {csv_path}\")\n",
        "\n",
        "# Optional Spark write (if Spark available and lakehouse_output_table provided)\n",
        "if SparkSession is not None and config.lakehouse_output_table:\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    sdf = spark.createDataFrame(result_df)\n",
        "    sdf.write.mode('overwrite').saveAsTable(config.lakehouse_output_table)\n",
        "    print(f\"Saved Spark table: {config.lakehouse_output_table}\")\n",
        "\n",
        "# Optional SQL Server write\n",
        "if config.enable_sql_write:\n",
        "    assert sa is not None, \"sqlalchemy required for SQL write\"\n",
        "    assert config.sql_connection_string and config.sql_target_table, \"Provide SQL connection and target table\"\n",
        "    engine = sa.create_engine(config.sql_connection_string, fast_executemany=True)\n",
        "    with engine.begin() as conn:\n",
        "        result_df.to_sql(name=config.sql_target_table.split('.')[-1],\n",
        "                         con=conn,\n",
        "                         schema=config.sql_target_table.split('.')[0] if '.' in config.sql_target_table else None,\n",
        "                         if_exists='replace',\n",
        "                         index=False,\n",
        "                         dtype=None)\n",
        "    print(f\"Wrote to SQL Server table {config.sql_target_table}\")\n",
        ""
      ],
      "execution_count": null,
      "outputs": [],
      "id": "bccbb300"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}